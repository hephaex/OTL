# OTL Development Environment
#
# Usage:
#   docker compose up -d
#   docker compose logs -f
#   docker compose down -v  (to remove volumes)

services:
  # ==========================================================================
  # SurrealDB - Graph + Document Database
  # ==========================================================================
  surrealdb:
    image: surrealdb/surrealdb:v2.4.0
    container_name: otl-surrealdb
    user: root
    command: start --log trace --user root --pass root surrealkv:/data/srdb.db
    ports:
      - "8001:8000"  # Changed to avoid port conflict with ntimes-coordinator
    volumes:
      - surrealdb_data:/data
    environment:
      - SURREAL_LOG=trace
    healthcheck:
      test: ["CMD", "/surreal", "isready"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ==========================================================================
  # Qdrant - Vector Database
  # ==========================================================================
  qdrant:
    image: qdrant/qdrant:v1.16.0
    container_name: otl-qdrant
    ports:
      - "6333:6333"  # HTTP API
      - "6334:6334"  # gRPC API
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__LOG_LEVEL=INFO
    healthcheck:
      test: ["CMD-SHELL", "bash -c '</dev/tcp/localhost/6333'"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ==========================================================================
  # PostgreSQL - Metadata + ACL
  # ==========================================================================
  postgres:
    image: postgres:16-alpine
    container_name: otl-postgres
    ports:
      - "5433:5432"  # Changed to avoid port conflict with casterhub-postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init.sql
    environment:
      POSTGRES_USER: otl
      POSTGRES_PASSWORD: otl_dev_password
      POSTGRES_DB: otl
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U otl"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ==========================================================================
  # Meilisearch - Full-text Search (Optional)
  # ==========================================================================
  meilisearch:
    image: getmeili/meilisearch:v1.10
    container_name: otl-meilisearch
    ports:
      - "7700:7700"
    volumes:
      - meilisearch_data:/meili_data
    environment:
      - MEILI_ENV=development
      - MEILI_MASTER_KEY=otl_meili_dev_key
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7700/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ==========================================================================
  # vLLM - High Performance LLM Inference Server
  # ==========================================================================
  vllm:
    image: vllm/vllm-openai:latest
    container_name: otl-vllm
    ports:
      - "8080:8000"  # OpenAI-compatible API
    volumes:
      - vllm_cache:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
    command: >
      --model Qwen/Qwen2.5-7B-Instruct
      --served-model-name qwen2.5-7b
      --max-model-len 8192
      --trust-remote-code
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 120s
    profiles:
      - vllm  # Only start with --profile vllm (requires NVIDIA GPU)
    # GPU is required for vLLM - use docker-compose.gpu.yml
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # ==========================================================================
  # Ollama - Local LLM (Default, supports Metal on macOS)
  # ==========================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: otl-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
    # GPU support: uncomment for NVIDIA GPU acceleration
    # Requires: nvidia-container-toolkit installed on host
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

volumes:
  surrealdb_data:
  qdrant_data:
  postgres_data:
  meilisearch_data:
  ollama_data:
  vllm_cache:

networks:
  default:
    name: otl-network
